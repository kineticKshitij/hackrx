# -*- coding: utf-8 -*-
"""BajajHackrx.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Er-eOXYqfgbL5gMokBfXXNr00ckDOwRi
"""

import os
import json
import asyncio
import logging
from typing import List, Dict, Any, Optional
from datetime import datetime
import requests
from io import BytesIO
import hashlib

# Load environment variables from .env file
from dotenv import load_dotenv
load_dotenv()

# FastAPI and async components
from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel, HttpUrl
import uvicorn

# Document processing
import PyPDF2
from docx import Document as DocxDocument
import email
from email.mime.text import MIMEText
import re

# ML and embedding components
import numpy as np
from sentence_transformers import SentenceTransformer

# Azure services
from azure.search.documents import SearchClient
from azure.search.documents.indexes import SearchIndexClient
from azure.search.documents.models import VectorizedQuery
from azure.core.credentials import AzureKeyCredential
from azure.search.documents.indexes.models import (
    SearchIndex, SearchField, SearchFieldDataType, SimpleField,
    SearchableField, VectorSearch, VectorSearchProfile,
    HnswAlgorithmConfiguration, VectorSearchAlgorithmConfiguration
)
import openai
from openai import AsyncAzureOpenAI

# Azure SQL Database
import pyodbc
import asyncio
import aioodbc

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Azure Configuration
class AzureConfig:
    # Azure OpenAI Configuration
    AZURE_OPENAI_ENDPOINT = os.getenv("AZURE_OPENAI_ENDPOINT", "https://your-resource.openai.azure.com/")
    AZURE_OPENAI_API_KEY = os.getenv("AZURE_OPENAI_API_KEY", "your-azure-openai-key")
    AZURE_OPENAI_API_VERSION = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview")
    AZURE_OPENAI_MODEL_DEPLOYMENT = os.getenv("AZURE_OPENAI_MODEL_DEPLOYMENT", "gpt-4")
    AZURE_OPENAI_EMBEDDING_DEPLOYMENT = os.getenv("AZURE_OPENAI_EMBEDDING_DEPLOYMENT", "text-embedding-ada-002")

    # Azure Cognitive Search Configuration
    AZURE_SEARCH_ENDPOINT = os.getenv("AZURE_SEARCH_ENDPOINT", "https://your-search-service.search.windows.net")
    AZURE_SEARCH_API_KEY = os.getenv("AZURE_SEARCH_API_KEY", "your-search-key")
    AZURE_SEARCH_INDEX_NAME = os.getenv("AZURE_SEARCH_INDEX_NAME", "document-chunks")

    # Azure SQL Database Configuration
    AZURE_SQL_SERVER = os.getenv("AZURE_SQL_SERVER", "your-server.database.windows.net")
    AZURE_SQL_DATABASE = os.getenv("AZURE_SQL_DATABASE", "your-database")
    AZURE_SQL_USERNAME = os.getenv("AZURE_SQL_USERNAME", "your-username")
    AZURE_SQL_PASSWORD = os.getenv("AZURE_SQL_PASSWORD", "your-password")
    AZURE_SQL_DRIVER = os.getenv("AZURE_SQL_DRIVER", "ODBC Driver 18 for SQL Server")

    # System Configuration
    BEARER_TOKEN = "d691ab348b0d57d77e97cb3d989203e9168c6f8a88e91dd37dc80ff0a9b213aa"
    CHUNK_SIZE = 512
    CHUNK_OVERLAP = 50
    TOP_K_RETRIEVAL = 5
    SIMILARITY_THRESHOLD = 0.7
    EMBEDDING_DIMENSIONS = 1536  # Ada-002 embedding dimensions

config = AzureConfig()

# Pydantic models
class DocumentInput(BaseModel):
    documents: str  # URL to document
    questions: List[str]

class QueryResponse(BaseModel):
    answers: List[str]
    metadata: Optional[Dict[str, Any]] = None

class ClauseMatch(BaseModel):
    clause_text: str
    similarity_score: float
    page_number: Optional[int] = None
    section: Optional[str] = None

# Security
security = HTTPBearer()

def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    if credentials.credentials != config.BEARER_TOKEN:
        raise HTTPException(status_code=401, detail="Invalid token")
    return credentials.credentials

# Document Processing Classes (same as before)
class DocumentProcessor:
    """Base class for document processing"""

    def __init__(self):
        self.supported_formats = ['.pdf', '.docx', '.eml', '.txt']

    def extract_text(self, file_content: bytes, file_type: str) -> str:
        """Extract text from different document formats"""
        try:
            if file_type.lower() == '.pdf':
                return self._extract_pdf_text(file_content)
            elif file_type.lower() == '.docx':
                return self._extract_docx_text(file_content)
            elif file_type.lower() == '.eml':
                return self._extract_email_text(file_content)
            elif file_type.lower() == '.txt':
                return file_content.decode('utf-8')
            else:
                raise ValueError(f"Unsupported file type: {file_type}")
        except Exception as e:
            logger.error(f"Error extracting text from {file_type}: {str(e)}")
            raise

    def _extract_pdf_text(self, file_content: bytes) -> str:
        """Extract text from PDF"""
        text = ""
        try:
            pdf_file = BytesIO(file_content)
            pdf_reader = PyPDF2.PdfReader(pdf_file)

            for page_num, page in enumerate(pdf_reader.pages):
                page_text = page.extract_text()
                text += f"\n[Page {page_num + 1}]\n{page_text}\n"
        except Exception as e:
            logger.error(f"PDF extraction error: {str(e)}")
            raise

        return text

    def _extract_docx_text(self, file_content: bytes) -> str:
        """Extract text from DOCX"""
        try:
            docx_file = BytesIO(file_content)
            doc = DocxDocument(docx_file)

            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"

            return text
        except Exception as e:
            logger.error(f"DOCX extraction error: {str(e)}")
            raise

    def _extract_email_text(self, file_content: bytes) -> str:
        """Extract text from email"""
        try:
            email_message = email.message_from_bytes(file_content)

            text = f"Subject: {email_message.get('Subject', 'No Subject')}\n"
            text += f"From: {email_message.get('From', 'Unknown')}\n"
            text += f"To: {email_message.get('To', 'Unknown')}\n\n"

            if email_message.is_multipart():
                for part in email_message.walk():
                    if part.get_content_type() == "text/plain":
                        text += part.get_payload(decode=True).decode('utf-8')
            else:
                text += email_message.get_payload(decode=True).decode('utf-8')

            return text
        except Exception as e:
            logger.error(f"Email extraction error: {str(e)}")
            raise

class TextChunker:
    """Handles text chunking for better retrieval"""

    def __init__(self, chunk_size: int = 512, overlap: int = 50):
        self.chunk_size = chunk_size
        self.overlap = overlap

    def chunk_text(self, text: str, metadata: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """Split text into overlapping chunks"""
        # Clean text
        text = self._clean_text(text)

        # Split by sentences first
        sentences = re.split(r'(?<=[.!?])\s+', text)

        chunks = []
        current_chunk = ""
        current_length = 0

        for sentence in sentences:
            sentence_length = len(sentence)

            if current_length + sentence_length > self.chunk_size:
                if current_chunk:
                    chunk_data = {
                        'text': current_chunk.strip(),
                        'metadata': metadata or {},
                        'length': current_length,
                        'chunk_id': len(chunks)
                    }
                    chunks.append(chunk_data)

                # Start new chunk with overlap
                if len(chunks) > 0:
                    overlap_text = current_chunk[-self.overlap:] if len(current_chunk) > self.overlap else current_chunk
                    current_chunk = overlap_text + " " + sentence
                    current_length = len(current_chunk)
                else:
                    current_chunk = sentence
                    current_length = sentence_length
            else:
                current_chunk += " " + sentence if current_chunk else sentence
                current_length += sentence_length

        # Add final chunk
        if current_chunk:
            chunk_data = {
                'text': current_chunk.strip(),
                'metadata': metadata or {},
                'length': current_length,
                'chunk_id': len(chunks)
            }
            chunks.append(chunk_data)

        return chunks

    def _clean_text(self, text: str) -> str:
        """Clean and normalize text"""
        # Remove extra whitespace
        text = re.sub(r'\s+', ' ', text)
        # Remove special characters but keep punctuation
        text = re.sub(r'[^\w\s.,!?;:()\-\[\]{}"\']', ' ', text)
        return text.strip()

class AzureEmbeddingManager:
    """Manages Azure OpenAI embeddings and Azure Cognitive Search"""

    def __init__(self):
        self.azure_client = AsyncAzureOpenAI(
            api_key=config.AZURE_OPENAI_API_KEY,
            api_version=config.AZURE_OPENAI_API_VERSION,
            azure_endpoint=config.AZURE_OPENAI_ENDPOINT
        )

        # Azure Cognitive Search clients
        self.search_client = SearchClient(
            endpoint=config.AZURE_SEARCH_ENDPOINT,
            index_name=config.AZURE_SEARCH_INDEX_NAME,
            credential=AzureKeyCredential(config.AZURE_SEARCH_API_KEY)
        )

        self.index_client = SearchIndexClient(
            endpoint=config.AZURE_SEARCH_ENDPOINT,
            credential=AzureKeyCredential(config.AZURE_SEARCH_API_KEY)
        )

        self.index_created = False

    async def create_search_index(self):
        """Create Azure Cognitive Search index with vector search capabilities"""
        if self.index_created:
            return

        try:
            # Define the search index with vector search
            fields = [
                SimpleField(name="id", type=SearchFieldDataType.String, key=True),
                SearchableField(name="content", type=SearchFieldDataType.String),
                SearchableField(name="document_url", type=SearchFieldDataType.String),
                SimpleField(name="chunk_id", type=SearchFieldDataType.Int32),
                SimpleField(name="page_number", type=SearchFieldDataType.Int32),
                SearchField(
                    name="content_vector",
                    type=SearchFieldDataType.Collection(SearchFieldDataType.Single),
                    searchable=True,
                    vector_search_dimensions=config.EMBEDDING_DIMENSIONS,
                    vector_search_profile_name="my-vector-config"
                ),
                SimpleField(name="metadata", type=SearchFieldDataType.String),
                SimpleField(name="created_at", type=SearchFieldDataType.DateTimeOffset)
            ]

            # Configure vector search
            vector_search = VectorSearch(
                profiles=[
                    VectorSearchProfile(
                        name="my-vector-config",
                        algorithm_configuration_name="my-hnsw-config"
                    )
                ],
                algorithms=[
                    HnswAlgorithmConfiguration(
                        name="my-hnsw-config"
                    )
                ]
            )

            # Create the search index
            index = SearchIndex(
                name=config.AZURE_SEARCH_INDEX_NAME,
                fields=fields,
                vector_search=vector_search
            )

            # Create or update the index
            self.index_client.create_or_update_index(index)
            self.index_created = True
            logger.info(f"Created/updated search index: {config.AZURE_SEARCH_INDEX_NAME}")

        except Exception as e:
            logger.error(f"Error creating search index: {str(e)}")
            # Continue without vector search if index creation fails
            self.index_created = True

    async def create_embeddings(self, texts: List[str]) -> List[List[float]]:
        """Create embeddings using Azure OpenAI"""
        try:
            # Azure OpenAI embedding API call
            response = await self.azure_client.embeddings.create(
                model=config.AZURE_OPENAI_EMBEDDING_DEPLOYMENT,
                input=texts
            )

            embeddings = [data.embedding for data in response.data]
            return embeddings

        except Exception as e:
            logger.error(f"Error creating embeddings: {str(e)}")
            raise

    async def index_chunks(self, chunks: List[Dict[str, Any]], document_url: str):
        """Index chunks in Azure Cognitive Search"""
        await self.create_search_index()

        try:
            # Prepare texts for embedding
            texts = [chunk['text'] for chunk in chunks]

            # Create embeddings
            embeddings = await self.create_embeddings(texts)

            # Prepare documents for indexing
            documents = []
            for i, (chunk, embedding) in enumerate(zip(chunks, embeddings)):
                doc_id = f"{hashlib.md5(document_url.encode()).hexdigest()}_{i}"

                document = {
                    "id": doc_id,
                    "content": chunk['text'],
                    "document_url": document_url,
                    "chunk_id": chunk.get('chunk_id', i),
                    "page_number": self._extract_page_number(chunk['text']),
                    "content_vector": embedding,
                    "metadata": json.dumps(chunk.get('metadata', {})),
                    "created_at": datetime.now().isoformat() + "Z"
                }
                documents.append(document)

            # Upload documents to search index
            result = self.search_client.upload_documents(documents)

            successful_uploads = sum(1 for r in result if r.succeeded)
            logger.info(f"Successfully indexed {successful_uploads}/{len(documents)} chunks")

        except Exception as e:
            logger.error(f"Error indexing chunks: {str(e)}")
            raise

    def _extract_page_number(self, text: str) -> int:
        """Extract page number from text if available"""
        match = re.search(r'\[Page (\d+)\]', text)
        return int(match.group(1)) if match else 1

    async def search_similar(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """Search for similar chunks using vector search and hybrid search"""
        try:
            # Create embedding for the query
            query_embeddings = await self.create_embeddings([query])
            query_vector = query_embeddings[0]

            # Perform hybrid search (vector + text)
            vector_query = VectorizedQuery(
                vector=query_vector,
                k_nearest_neighbors=k,
                fields="content_vector"
            )

            results = self.search_client.search(
                search_text=query,
                vector_queries=[vector_query],
                select=["id", "content", "document_url", "chunk_id", "page_number", "metadata"],
                top=k
            )

            # Process results
            search_results = []
            for result in results:
                search_result = {
                    'chunk': {
                        'text': result['content'],
                        'metadata': json.loads(result.get('metadata', '{}')),
                        'chunk_id': result.get('chunk_id', 0),
                        'page_number': result.get('page_number', 1)
                    },
                    'similarity_score': result.get('@search.score', 0.0),
                    'document_url': result.get('document_url', ''),
                    'rank': len(search_results) + 1
                }
                search_results.append(search_result)

            return search_results

        except Exception as e:
            logger.error(f"Error in similarity search: {str(e)}")
            return []

class AzureLLMQueryProcessor:
    """Handles Azure OpenAI-based query processing and response generation"""

    def __init__(self):
        self.client = AsyncAzureOpenAI(
            api_key=config.AZURE_OPENAI_API_KEY,
            api_version=config.AZURE_OPENAI_API_VERSION,
            azure_endpoint=config.AZURE_OPENAI_ENDPOINT
        )
        self.model = config.AZURE_OPENAI_MODEL_DEPLOYMENT

    async def extract_structured_query(self, query: str) -> Dict[str, Any]:
        """Extract structured information from natural language query"""

        prompt = f"""
        Analyze the following query and extract structured information:

        Query: "{query}"

        Please return a JSON object with the following structure:
        {{
            "intent": "information_retrieval|coverage_check|condition_lookup|benefit_inquiry",
            "entities": {{
                "medical_procedure": "extracted medical procedure or condition",
                "policy_aspect": "coverage|waiting_period|premium|benefit|condition",
                "keywords": ["list", "of", "relevant", "keywords"]
            }},
            "question_type": "yes_no|factual|conditional|comparative",
            "complexity": "simple|medium|complex"
        }}
        """

        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are an expert at analyzing insurance and policy queries. Return only valid JSON."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.1,
                max_tokens=300
            )

            result = json.loads(response.choices[0].message.content)
            return result

        except Exception as e:
            logger.error(f"Error in structured query extraction: {str(e)}")
            # Return fallback structure
            return {
                "intent": "information_retrieval",
                "entities": {"keywords": query.split()},
                "question_type": "factual",
                "complexity": "medium"
            }

    async def generate_answer(self, query: str, relevant_chunks: List[Dict[str, Any]],
                            structured_query: Dict[str, Any]) -> str:
        """Generate answer based on retrieved chunks using Azure OpenAI"""

        # Prepare context from chunks
        context = ""
        for i, chunk_data in enumerate(relevant_chunks):
            chunk = chunk_data['chunk']
            score = chunk_data.get('similarity_score', 0.0)
            page = chunk.get('page_number', 'Unknown')
            context += f"\n[Context {i+1}] (Relevance: {score:.3f}, Page: {page})\n{chunk['text']}\n"

        # Create prompt based on query complexity
        if structured_query.get('question_type') == 'yes_no':
            prompt_template = """
            Based on the provided policy context, answer the following yes/no question clearly and provide reasoning.

            Question: {query}

            Context:
            {context}

            Please provide:
            1. A clear YES or NO answer
            2. Specific policy details that support your answer
            3. Any relevant conditions or limitations
            4. Reference to specific clauses if mentioned

            Format your response as a comprehensive but concise answer.
            """
        else:
            prompt_template = """
            Based on the provided policy context, answer the following question comprehensively.

            Question: {query}

            Context:
            {context}

            Please provide:
            1. A direct answer to the question
            2. Specific details from the policy
            3. Any relevant conditions, limitations, or requirements
            4. Reference to specific clauses or sections when applicable

            Be precise and factual. If the context doesn't contain enough information, state that clearly.
            """

        prompt = prompt_template.format(query=query, context=context)

        try:
            response = await self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "You are an expert insurance policy analyst. Provide accurate, detailed answers based strictly on the provided context. Always cite specific policy details when available."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.2,
                max_tokens=500
            )

            return response.choices[0].message.content.strip()

        except Exception as e:
            logger.error(f"Error generating answer: {str(e)}")
            return f"I apologize, but I encountered an error while processing your question: {query}. Please try again."

class AzureSQLDatabase:
    """Azure SQL Database management for document storage"""

    def __init__(self):
        self.connection_string = (
            f"DRIVER={{{config.AZURE_SQL_DRIVER}}};"
            f"SERVER={config.AZURE_SQL_SERVER};"
            f"DATABASE={config.AZURE_SQL_DATABASE};"
            f"UID={config.AZURE_SQL_USERNAME};"
            f"PWD={config.AZURE_SQL_PASSWORD};"
            f"Encrypt=yes;"
            f"TrustServerCertificate=no;"
            f"Connection Timeout=30;"
        )
        self.init_database()

    def init_database(self):
        """Initialize Azure SQL Database tables"""
        try:
            conn = pyodbc.connect(self.connection_string)
            cursor = conn.cursor()

            # Create documents table
            cursor.execute("""
                IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='documents' AND xtype='U')
                CREATE TABLE documents (
                    id INT IDENTITY(1,1) PRIMARY KEY,
                    url NVARCHAR(500) UNIQUE,
                    content_hash NVARCHAR(100),
                    processed_at DATETIME2 DEFAULT GETDATE(),
                    chunk_count INT,
                    metadata NVARCHAR(MAX),
                    file_type NVARCHAR(50),
                    file_size BIGINT
                )
            """)

            # Create queries table
            cursor.execute("""
                IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='queries' AND xtype='U')
                CREATE TABLE queries (
                    id INT IDENTITY(1,1) PRIMARY KEY,
                    query_text NVARCHAR(MAX),
                    document_id INT,
                    response NVARCHAR(MAX),
                    processing_time FLOAT,
                    similarity_scores NVARCHAR(MAX),
                    created_at DATETIME2 DEFAULT GETDATE(),
                    FOREIGN KEY (document_id) REFERENCES documents (id)
                )
            """)

            # Create performance tracking table
            cursor.execute("""
                IF NOT EXISTS (SELECT * FROM sysobjects WHERE name='performance_metrics' AND xtype='U')
                CREATE TABLE performance_metrics (
                    id INT IDENTITY(1,1) PRIMARY KEY,
                    operation_type NVARCHAR(50),
                    duration_ms INT,
                    tokens_used INT,
                    success BIT,
                    error_message NVARCHAR(MAX),
                    created_at DATETIME2 DEFAULT GETDATE()
                )
            """)

            conn.commit()
            conn.close()
            logger.info("Azure SQL Database tables initialized successfully")

        except Exception as e:
            logger.error(f"Error initializing Azure SQL Database: {str(e)}")
            raise

    def store_document(self, url: str, content_hash: str, chunk_count: int,
                      metadata: Dict = None, file_type: str = None, file_size: int = None):
        """Store document metadata in Azure SQL"""
        try:
            conn = pyodbc.connect(self.connection_string)
            cursor = conn.cursor()

            cursor.execute("""
                MERGE documents AS target
                USING (SELECT ? AS url) AS source
                ON target.url = source.url
                WHEN MATCHED THEN
                    UPDATE SET content_hash = ?, chunk_count = ?, metadata = ?,
                              file_type = ?, file_size = ?, processed_at = GETDATE()
                WHEN NOT MATCHED THEN
                    INSERT (url, content_hash, chunk_count, metadata, file_type, file_size)
                    VALUES (?, ?, ?, ?, ?, ?);
            """, (url, content_hash, chunk_count, json.dumps(metadata or {}),
                  file_type, file_size, url, content_hash, chunk_count,
                  json.dumps(metadata or {}), file_type, file_size))

            conn.commit()

            # Get the document ID
            cursor.execute("SELECT id FROM documents WHERE url = ?", (url,))
            doc_id = cursor.fetchone()[0]

            conn.close()
            return doc_id

        except Exception as e:
            logger.error(f"Error storing document in Azure SQL: {str(e)}")
            raise

    def get_document(self, url: str) -> Optional[Dict]:
        """Retrieve document metadata from Azure SQL"""
        try:
            conn = pyodbc.connect(self.connection_string)
            cursor = conn.cursor()

            cursor.execute("""
                SELECT id, url, content_hash, processed_at, chunk_count, metadata, file_type, file_size
                FROM documents WHERE url = ?
            """, (url,))

            row = cursor.fetchone()
            conn.close()

            if row:
                return {
                    'id': row[0],
                    'url': row[1],
                    'content_hash': row[2],
                    'processed_at': row[3],
                    'chunk_count': row[4],
                    'metadata': json.loads(row[5]) if row[5] else {},
                    'file_type': row[6],
                    'file_size': row[7]
                }
            return None

        except Exception as e:
            logger.error(f"Error retrieving document from Azure SQL: {str(e)}")
            return None

    def log_query(self, query_text: str, document_id: int, response: str,
                  processing_time: float, similarity_scores: List[float] = None):
        """Log query and response for analytics"""
        try:
            conn = pyodbc.connect(self.connection_string)
            cursor = conn.cursor()

            cursor.execute("""
                INSERT INTO queries (query_text, document_id, response, processing_time, similarity_scores)
                VALUES (?, ?, ?, ?, ?)
            """, (query_text, document_id, response, processing_time,
                  json.dumps(similarity_scores) if similarity_scores else None))

            conn.commit()
            conn.close()

        except Exception as e:
            logger.error(f"Error logging query to Azure SQL: {str(e)}")

# Main Azure-powered Retrieval System
class AzureIntelligentRetrievalSystem:
    """Main system orchestrator using Azure services"""

    def __init__(self):
        self.doc_processor = DocumentProcessor()
        self.chunker = TextChunker(config.CHUNK_SIZE, config.CHUNK_OVERLAP)
        self.embedding_manager = AzureEmbeddingManager()
        self.llm_processor = AzureLLMQueryProcessor()
        self.database = AzureSQLDatabase()

        # Cache for processed documents
        self.document_cache = {}

    async def process_document(self, document_url: str) -> bool:
        """Download and process document using Azure services"""
        try:
            start_time = datetime.now()

            # Check if document already processed
            doc_info = self.database.get_document(document_url)

            # Download document
            response = requests.get(document_url, timeout=30)
            response.raise_for_status()

            content = response.content
            content_hash = hashlib.md5(content).hexdigest()
            file_size = len(content)

            # Check if content changed
            if doc_info and doc_info['content_hash'] == content_hash:
                logger.info(f"Document already processed: {document_url}")
                return True

            # Determine file type
            file_type = self._get_file_type(document_url, response.headers)

            # Extract text
            text = self.doc_processor.extract_text(content, file_type)

            # Create chunks
            metadata = {
                'url': document_url,
                'file_type': file_type,
                'content_hash': content_hash,
                'file_size': file_size
            }
            chunks = self.chunker.chunk_text(text, metadata)

            # Index chunks in Azure Cognitive Search
            await self.embedding_manager.index_chunks(chunks, document_url)

            # Store in Azure SQL Database
            doc_id = self.database.store_document(
                document_url, content_hash, len(chunks), metadata, file_type, file_size
            )

            # Cache the processed document
            self.document_cache[document_url] = {
                'chunks': chunks,
                'processed_at': datetime.now(),
                'doc_id': doc_id
            }

            processing_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"Successfully processed document: {document_url} with {len(chunks)} chunks in {processing_time:.2f}s")
            return True

        except Exception as e:
            logger.error(f"Error processing document {document_url}: {str(e)}")
            return False

    def _get_file_type(self, url: str, headers: Dict) -> str:
        """Determine file type from URL or headers"""
        # Check URL extension
        for ext in ['.pdf', '.docx', '.eml', '.txt']:
            if url.lower().endswith(ext):
                return ext

        # Check content type header
        content_type = headers.get('content-type', '').lower()
        if 'pdf' in content_type:
            return '.pdf'
        elif 'word' in content_type or 'officedocument' in content_type:
            return '.docx'
        elif 'email' in content_type:
            return '.eml'
        else:
            return '.txt'  # Default fallback

    async def query_document(self, query: str, document_url: str = None) -> str:
        """Process a query against the loaded document using Azure services"""
        try:
            start_time = datetime.now()

            # Extract structured query information
            structured_query = await self.llm_processor.extract_structured_query(query)

            # Enhance query with extracted keywords
            enhanced_query = query
            if 'entities' in structured_query and 'keywords' in structured_query['entities']:
                keywords = structured_query['entities']['keywords']
                enhanced_query = f"{query} {' '.join(keywords)}"

            # Retrieve relevant chunks using Azure Cognitive Search
            relevant_chunks = await self.embedding_manager.search_similar(
                enhanced_query,
                k=config.TOP_K_RETRIEVAL
            )

            # Filter by similarity threshold
            filtered_chunks = [
                chunk for chunk in relevant_chunks
                if chunk['similarity_score'] >= config.SIMILARITY_THRESHOLD
            ]

            if not filtered_chunks:
                return "I couldn't find relevant information in the document to answer your question."

            # Generate answer using Azure OpenAI
            answer = await self.llm_processor.generate_answer(
                query, filtered_chunks, structured_query
            )

            processing_time = (datetime.now() - start_time).total_seconds()

            # Log query for analytics
            if document_url and document_url in self.document_cache:
                doc_id = self.document_cache[document_url]['doc_id']
                similarity_scores = [chunk['similarity_score'] for chunk in filtered_chunks]
                self.database.log_query(query, doc_id, answer, processing_time, similarity_scores)

            logger.info(f"Query processed in {processing_time:.2f} seconds")

            return answer

        except Exception as e:
            logger.error(f"Error processing query '{query}': {str(e)}")
            return f"I encountered an error while processing your question. Please try again."

# FastAPI Application with Azure Integration
app = FastAPI(
    title="Azure-Powered LLM Intelligent Query-Retrieval System",
    description="Document processing and intelligent query system using Azure OpenAI, Azure Cognitive Search, and Azure SQL Database",
    version="2.0.0"
)

# Global system instance
retrieval_system = AzureIntelligentRetrievalSystem()

@app.post("/hackrx/run", response_model=QueryResponse)
async def run_submission(
    request: DocumentInput,
    background_tasks: BackgroundTasks,
    token: str = Depends(verify_token)
):
    """Main endpoint for processing documents and answering queries using Azure services"""

    try:
        # Process document
        success = await retrieval_system.process_document(request.documents)

        if not success:
            raise HTTPException(
                status_code=400,
                detail="Failed to process the provided document"
            )

        # Process all queries
        answers = []
        processing_times = []

        for query in request.questions:
            start_time = datetime.now()
            answer = await retrieval_system.query_document(query, request.documents)
            processing_time = (datetime.now() - start_time).total_seconds()

            answers.append(answer)
            processing_times.append(processing_time)

        # Prepare response with Azure-specific metadata
        response = QueryResponse(
            answers=answers,
            metadata={
                "document_url": request.documents,
                "total_questions": len(request.questions),
                "processing_timestamp": datetime.now().isoformat(),
                "average_processing_time": sum(processing_times) / len(processing_times),
                "azure_services_used": [
                    "Azure OpenAI Service",
                    "Azure Cognitive Search",
                    "Azure SQL Database"
                ],
                "embedding_model": config.AZURE_OPENAI_EMBEDDING_DEPLOYMENT,
                "llm_model": config.AZURE_OPENAI_MODEL_DEPLOYMENT
            }
        )

        return response

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error in run_submission: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Internal server error: {str(e)}")

@app.get("/health")
async def health_check():
    """Health check endpoint with Azure services status"""
    try:
        # Test Azure OpenAI connection
        azure_openai_status = "healthy"
        try:
            client = AsyncAzureOpenAI(
                api_key=config.AZURE_OPENAI_API_KEY,
                api_version=config.AZURE_OPENAI_API_VERSION,
                azure_endpoint=config.AZURE_OPENAI_ENDPOINT
            )
            # Simple test call
            await client.chat.completions.create(
                model=config.AZURE_OPENAI_MODEL_DEPLOYMENT,
                messages=[{"role": "user", "content": "test"}],
                max_tokens=1
            )
        except:
            azure_openai_status = "unhealthy"

        # Test Azure Cognitive Search connection
        search_status = "healthy"
        try:
            search_client = SearchClient(
                endpoint=config.AZURE_SEARCH_ENDPOINT,
                index_name=config.AZURE_SEARCH_INDEX_NAME,
                credential=AzureKeyCredential(config.AZURE_SEARCH_API_KEY)
            )
            # Simple search test
            list(search_client.search("test", top=1))
        except:
            search_status = "unhealthy"

        # Test Azure SQL Database connection
        sql_status = "healthy"
        try:
            conn = pyodbc.connect(retrieval_system.database.connection_string)
            cursor = conn.cursor()
            cursor.execute("SELECT 1")
            conn.close()
        except:
            sql_status = "unhealthy"

        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "azure_services": {
                "azure_openai": azure_openai_status,
                "azure_cognitive_search": search_status,
                "azure_sql_database": sql_status
            },
            "version": "2.0.0"
        }

    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@app.get("/")
async def root():
    """Root endpoint with Azure API information"""
    return {
        "message": "Azure-Powered LLM Intelligent Query-Retrieval System",
        "version": "2.0.0",
        "azure_services": [
            "Azure OpenAI Service",
            "Azure Cognitive Search",
            "Azure SQL Database"
        ],
        "endpoints": {
            "main": "/hackrx/run",
            "health": "/health",
            "docs": "/docs",
            "metrics": "/metrics"
        },
        "features": [
            "Vector search with Azure Cognitive Search",
            "GPT-4 powered query understanding",
            "Azure SQL Database for analytics",
            "Hybrid search capabilities",
            "Enterprise-grade security"
        ]
    }

@app.get("/metrics")
async def get_metrics():
    """Get system performance metrics from Azure SQL Database"""
    try:
        conn = pyodbc.connect(retrieval_system.database.connection_string)
        cursor = conn.cursor()

        # Get recent performance metrics
        cursor.execute("""
            SELECT
                operation_type,
                AVG(duration_ms) as avg_duration,
                COUNT(*) as total_operations,
                SUM(CASE WHEN success = 1 THEN 1 ELSE 0 END) as successful_operations
            FROM performance_metrics
            WHERE created_at >= DATEADD(hour, -24, GETDATE())
            GROUP BY operation_type
        """)

        performance_data = []
        for row in cursor.fetchall():
            performance_data.append({
                'operation_type': row[0],
                'avg_duration_ms': float(row[1]) if row[1] else 0,
                'total_operations': row[2],
                'successful_operations': row[3],
                'success_rate': (row[3] / row[2] * 100) if row[2] > 0 else 0
            })

        # Get query statistics
        cursor.execute("""
            SELECT
                COUNT(*) as total_queries,
                AVG(processing_time) as avg_processing_time,
                MAX(processing_time) as max_processing_time,
                MIN(processing_time) as min_processing_time
            FROM queries
            WHERE created_at >= DATEADD(hour, -24, GETDATE())
        """)

        query_stats = cursor.fetchone()
        conn.close()

        return {
            "timestamp": datetime.now().isoformat(),
            "period": "Last 24 hours",
            "performance_metrics": performance_data,
            "query_statistics": {
                "total_queries": query_stats[0] if query_stats[0] else 0,
                "avg_processing_time": float(query_stats[1]) if query_stats[1] else 0,
                "max_processing_time": float(query_stats[2]) if query_stats[2] else 0,
                "min_processing_time": float(query_stats[3]) if query_stats[3] else 0
            }
        }

    except Exception as e:
        logger.error(f"Error getting metrics: {str(e)}")
        return {"error": "Unable to retrieve metrics", "timestamp": datetime.now().isoformat()}

# Azure configuration validation
@app.on_event("startup")
async def startup_event():
    """Validate Azure configuration on startup"""
    logger.info("Starting Azure-Powered LLM Intelligent Query-Retrieval System...")

    # Validate Azure OpenAI configuration
    required_azure_configs = [
        "AZURE_OPENAI_ENDPOINT",
        "AZURE_OPENAI_API_KEY",
        "AZURE_SEARCH_ENDPOINT",
        "AZURE_SEARCH_API_KEY",
        "AZURE_SQL_SERVER",
        "AZURE_SQL_DATABASE",
        "AZURE_SQL_USERNAME",
        "AZURE_SQL_PASSWORD"
    ]

    missing_configs = []
    for config_name in required_azure_configs:
        if not getattr(config, config_name, None) or getattr(config, config_name).startswith("your-"):
            missing_configs.append(config_name)

    if missing_configs:
        logger.warning(f"Missing Azure configuration: {', '.join(missing_configs)}")
        logger.warning("Please set the required environment variables for full functionality")
    else:
        logger.info("Azure configuration validated successfully")

    # Initialize Azure Cognitive Search index
    try:
        await retrieval_system.embedding_manager.create_search_index()
        logger.info("Azure Cognitive Search index initialized")
    except Exception as e:
        logger.error(f"Failed to initialize Azure Cognitive Search: {str(e)}")

# Development server runner
if __name__ == "__main__":
    print("""
    =============================================================
    Azure-Powered LLM Intelligent Query-Retrieval System v2.0.0
    =============================================================

    Required Azure Services:
    1. Azure OpenAI Service
    2. Azure Cognitive Search
    3. Azure SQL Database

    Required Environment Variables:
    - AZURE_OPENAI_ENDPOINT
    - AZURE_OPENAI_API_KEY
    - AZURE_OPENAI_MODEL_DEPLOYMENT (default: gpt-4)
    - AZURE_OPENAI_EMBEDDING_DEPLOYMENT (default: text-embedding-ada-002)
    - AZURE_SEARCH_ENDPOINT
    - AZURE_SEARCH_API_KEY
    - AZURE_SEARCH_INDEX_NAME (default: document-chunks)
    - AZURE_SQL_SERVER
    - AZURE_SQL_DATABASE
    - AZURE_SQL_USERNAME
    - AZURE_SQL_PASSWORD

    Installing required packages...
    """)

    # Install required packages
    os.system("""
        pip install fastapi uvicorn sentence-transformers PyPDF2 python-docx requests numpy
        pip install azure-search-documents azure-identity azure-core
        pip install openai pyodbc aioodbc
    """)

    print("\nStarting server...")
    print("API will be available at: http://localhost:8000")
    print("API Documentation: http://localhost:8000/docs")
    print("Health Check: http://localhost:8000/health")
    print("Metrics: http://localhost:8000/metrics")

    uvicorn.run(
        "bajajhackrx:app",  # Updated to match the actual filename
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )